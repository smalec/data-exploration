{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_support(data, candidates, min_supp):\n",
    "    count_dict = {}\n",
    "    for transaction in data:\n",
    "        for candidate in candidates:\n",
    "            if not candidate in count_dict:\n",
    "                count_dict[candidate] = 0.0\n",
    "            if candidate.issubset(transaction):\n",
    "                count_dict[candidate] += 1\n",
    "    data_size = len(data)\n",
    "    filtered_candidates = set()\n",
    "    support_values = {}\n",
    "    for candidate in candidates:\n",
    "        support = count_dict[candidate] / data_size\n",
    "        if support >= min_supp:\n",
    "            filtered_candidates.add(candidate)\n",
    "            support_values[candidate] = support\n",
    "    return filtered_candidates, support_values\n",
    "\n",
    "def generate_candidates(prev_candidates):\n",
    "    k = len(random.sample(prev_candidates, 1)[0])\n",
    "    new_candidates = set()\n",
    "    for canA, canB in itertools.combinations(prev_candidates, 2):\n",
    "        listA, listB = sorted(list(canA))[:-1], sorted(list(canB))[:-1]\n",
    "        if listA == listB:\n",
    "            new_candidates.add(canA | canB)\n",
    "    return new_candidates\n",
    "\n",
    "def generateL1(data, min_supp):\n",
    "    unique_elements = {frozenset([elem]) for transaction in data for elem in transaction}\n",
    "    print \"Unique elements created:\", len(unique_elements)\n",
    "    return filter_support(data, unique_elements, min_supp)\n",
    "\n",
    "def apriori(data, min_supp):\n",
    "    L1, all_support_values = generateL1(data, min_supp)\n",
    "    print \"Generated L1\"\n",
    "    L = [L1]\n",
    "    k = 2\n",
    "    while L[k-2]:\n",
    "        candidates = generate_candidates(L[k-2])\n",
    "        filtered_candidates, support_values = filter_support(data, candidates, min_supp)\n",
    "        print \"Done step\", k-1\n",
    "        all_support_values.update(support_values)\n",
    "        L.append(filtered_candidates)\n",
    "        k += 1\n",
    "    all_support_values.update({frozenset([]): 1.0})\n",
    "    return reduce(lambda x, y: x | y, L), all_support_values \n",
    "\n",
    "def generate_reliable_rules(frequent_sets, support_values, min_confidence):\n",
    "    reliable_rules = set()\n",
    "    for set_ in frequent_sets:\n",
    "        S = [set([(set_, frozenset())])]\n",
    "        k = 1\n",
    "        while S[k-1]:\n",
    "            Sk = set()\n",
    "            for rule in S[-1]:\n",
    "                left, right = rule\n",
    "                for elem in left:\n",
    "                    left_copy = set(left)\n",
    "                    left_copy.remove(elem)\n",
    "                    confidence = support_values[set_] / support_values[frozenset(left_copy)]\n",
    "                    if confidence >= min_confidence:\n",
    "                        right_copy = set(right)\n",
    "                        right_copy.add(elem)\n",
    "                        Sk.add((frozenset(left_copy), frozenset(right_copy)))\n",
    "            S.append(Sk)\n",
    "            k += 1\n",
    "        reliable_rules |= reduce(lambda x, y: x | y, S)\n",
    "    return reliable_rules\n",
    "\n",
    "def print_rules(rules, support_values, min_lift=1.0, min_leverage=0.0):\n",
    "    \n",
    "    def print_set(set_):\n",
    "        set_string = \"{\"\n",
    "        for e in set_:\n",
    "            set_string += (str(e) + \", \")\n",
    "        if set_string[-1] != \"{\":\n",
    "            set_string = set_string[:-2]\n",
    "        return set_string + \"}\"\n",
    "    \n",
    "    for left, right in rules:\n",
    "        confidence = support_values[left | right] / support_values[left]\n",
    "        lift = confidence / support_values[right]\n",
    "        leverage = support_values[left | right] - support_values[left]*support_values[right]\n",
    "        if lift >= min_lift and leverage >= min_leverage:\n",
    "            print print_set(left) + \" => \" + print_set(right) + \",\",\n",
    "            print \"confidence: \" + str(confidence) + \",\",\n",
    "            print \"lift: \" + str(lift) + \",\",\n",
    "            print \"leverage: \" + str(leverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_random_dataset(size, max_items):\n",
    "    return {frozenset(random.sample(range(max_items), random.randint(1, max_items))) for _ in range(size)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique elements created: 15\n",
      "Generated L1\n",
      "Done step 1\n",
      "Done step 2\n",
      "Frequent sets: set([frozenset([1, 2]), frozenset([2]), frozenset([1])])\n"
     ]
    }
   ],
   "source": [
    "random_dataset = generate_random_dataset(10000, 15)\n",
    "modified_dataset = {t|frozenset([1, 2]) if random.random() < 0.75 else t for t in random_dataset}\n",
    "freq_sets = apriori(modified_dataset, 0.7)\n",
    "print \"Frequent sets:\", freq_sets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1} => {2}, confidence: 0.906862745098, lift: 1.0786099039, leverage: 0.0560314639148\n",
      "{2} => {1}, confidence: 0.91441207076, lift: 1.0786099039, leverage: 0.0560314639148\n"
     ]
    }
   ],
   "source": [
    "rules = generate_reliable_rules(freq_sets[0], freq_sets[1], 0.5)\n",
    "print_rules(rules, freq_sets[1], min_leverage=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dataset(filename):\n",
    "    data = set()\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line != \"\\n\":\n",
    "                data.add(frozenset(map(int, line.split())))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n",
      "Unique elements created: 16470\n",
      "Generated L1\n",
      "Done step 1\n",
      "Done step 2\n",
      "Done step 3\n",
      "Done step 4\n",
      "#frequent_sets: 169\n"
     ]
    }
   ],
   "source": [
    "dataset = read_dataset(\"retail.dat\")\n",
    "print \"Data loaded\"\n",
    "frequent_sets, support_values = apriori(dataset, 0.01)\n",
    "print \"#frequent_sets:\", len(frequent_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{36} => {38}, confidence: 0.95337911095, lift: 5.39754675346, leverage: 0.0257427170229\n",
      "{48, 41} => {39}, confidence: 0.815777627729, lift: 1.42309390178, leverage: 0.0258134972152\n",
      "{41, 39} => {48}, confidence: 0.652886607223, lift: 1.33798485118, leverage: 0.0219325777492\n",
      "{170} => {38}, confidence: 0.981362250085, lift: 5.55597302906, leverage: 0.0284436359895\n",
      "{110} => {38}, confidence: 0.973408541499, lift: 5.51094318368, leverage: 0.0236866636107\n",
      "{41} => {39}, confidence: 0.762097604503, lift: 1.3294510865, leverage: 0.0329552385461\n"
     ]
    }
   ],
   "source": [
    "rules = generate_reliable_rules(frequent_sets, support_values, 0.5)\n",
    "print_rules(rules, support_values, min_lift=1.3, min_leverage=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kosarak = read_dataset(\"kosarak.dat\")\n",
    "print \"Data loaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kosarak_frequent_sets1, kosarak_support_values1 = apriori(kosarak, 0.01)\n",
    "print \"#frequent_sets (1%):\", len(kosarak_frequent_sets1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kosarak_frequent_sets05, kosarak_support_values05 = apriori(kosarak, 0.005)\n",
    "print \"#frequent_sets (0.5%):\", len(kosarak_frequent_sets05)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
